{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core modules\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# data module\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# auxiliary modules\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"\n",
    "******************* Data Prep *****************\n",
    "original data set size: 25000\n",
    "12500 cats, 12500 dogs   \n",
    "microsoft asirre datasets cats and dogs (from kaggle competition)\n",
    "***********************************************\n",
    "\"\"\"\n",
    "\n",
    "base_dir = 'Datasets'\n",
    "base_dir = os.path.join(base_dir,'cats_and_dogs_25000')\n",
    "training_dir =os.path.join(base_dir,'train')\n",
    "validation_dir =os.path.join(base_dir,'validation')\n",
    "\n",
    "train_dir_cats = os.path.join(training_dir,'cats')\n",
    "train_dir_dogs = os.path.join(training_dir,'dogs')\n",
    "validation_dir_cats = os.path.join(validation_dir,'cats')\n",
    "validation_dir_dogs = os.path.join(validation_dir,'dogs')\n",
    "\n",
    "print(base_dir)\n",
    "print(training_dir)\n",
    "print(validation_dir)\n",
    "print(train_dir_cats)\n",
    "print(validation_dir_cats)\n",
    "\n",
    "# check basic statistics and number of files\n",
    "num_train_cats=len(os.listdir(train_dir_cats))\n",
    "print(\"Number of training files for cats : {}\".format(num_train_cats))\n",
    "num_train_dogs=len(os.listdir(train_dir_dogs))\n",
    "print(\"Number of training files for dogs : {}\".format(num_train_dogs))\n",
    "num_val_cats = len(os.listdir(validation_dir_cats))\n",
    "num_val_dogs = len(os.listdir(validation_dir_dogs))\n",
    "total_train=num_train_cats + num_train_dogs\n",
    "total_val = num_val_cats+num_val_dogs\n",
    "\n",
    "\"\"\"\n",
    "**************************  Image Preparation ****************\n",
    "Images must be formatted into pre-processed floating point tensors before being fed into the network\n",
    "\n",
    "Steps:\n",
    "Read files from disk\n",
    "Decode the contents of these images and convert them into a proper grid format as per their RGB content\n",
    "Convert them to floating point tensors\n",
    "Rescale the tensors from values between 0 and 255 to values between 0 and 1 \n",
    "(since NN work better with small input values) \n",
    "These can be done with the : tensorflow.keras.preprocessing.image.ImageDataGenerator\n",
    "\"\"\"\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "IMAGE_SHAPE =150\n",
    "training_image_generator = ImageDataGenerator(rescale =1/255)\n",
    "validation_image_generator = ImageDataGenerator(rescale =1/255)\n",
    "\n",
    "train_gen = training_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n",
    "                        directory=training_dir,shuffle=True,target_size=(IMAGE_SHAPE,IMAGE_SHAPE),class_mode='binary')\n",
    "validation_gen = validation_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n",
    "                        directory=validation_dir,shuffle=True,target_size=(IMAGE_SHAPE,IMAGE_SHAPE),class_mode='binary')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "NETWORK NETWORK DESIGN\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "layer0 = tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3))\n",
    "layer1 = tf.keras.layers.MaxPooling2D(2,2)\n",
    "layer2 = tf.keras.layers.Conv2D(64,(3,3),activation='relu')\n",
    "layer3 = tf.keras.layers.MaxPooling2D(2,2)\n",
    "layer4 = tf.keras.layers.Conv2D(128,(3,3),activation='relu')\n",
    "layer5 = tf.keras.layers.MaxPooling2D(2,2)\n",
    "layer6 = tf.keras.layers.Flatten()\n",
    "layer7 = tf.keras.layers.Dense(512,activation='relu')\n",
    "layer8 = tf.keras.layers.Dense(2,activation='softmax')\n",
    "\n",
    "model =tf.keras.Sequential([layer0,layer1,layer2,layer3,layer4,layer5,layer6,layer7,layer8])\n",
    "\n",
    "model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_gen,steps_per_epoch=math.ceil(total_train/BATCH_SIZE),epochs=100,validation_data=validation_gen,validation_steps=math.ceil(total_val/BATCH_SIZE))\n",
    "test_loss,test_accuracy = model.evaluate(validation_gen,steps=math.ceil(total_train/BATCH_SIZE))\n",
    "\n",
    "print(\"Model accuracy: {}\".format(test_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
